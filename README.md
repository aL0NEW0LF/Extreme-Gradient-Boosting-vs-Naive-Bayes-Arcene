# Extreme-Gradient-Boosting-vs-Naive-Bayes-Arcene

### EL GHAZI LOUBNA - FAKHRE-EDDINE MOHAMED AMINE - ZAOUI HANANE

**Subject:**  
Ovarian or prostate cancer   
Link: [https://archive.ics.uci.edu/ml/datasets/Arcene](https://archive.ics.uci.edu/ml/datasets/Arcene)  
Machine learning techniques: Extreme Gradient Boosting VS Naive Bayes  

**Goal**: Implementation of machine learning techniques, seen in the machine learning module, by applying them to example databases, taken from the "UCI Machine Learning Repository" and linked to different problems.   

The report covers the following elements:  
- [ ] Data description
- [ ] Summary of research on the subject, with priority given to the links at the bottom of the page.
- [ ] Possible pre-processing
- [ ] Implementation of machine learning techniques assigned to the group
- [ ] Presentation of results for the various metrics 
- [ ] Compare the results with those obtained from implementations provided by certain Python library modules.
- [ ] Conclusion
- [ ] Bibliography

**Important:**  
- [ ] Report: - Report (.doc/.pdf file) - Commented code **.ipynb** (Notebook Jupyter).
- [ ] The evaluation covers, among other things, the custom code provided and the improvements in machine learning algorithms taken into account, relative to standard variants.
Submission by Sunday 17/12/2023.  

**Tasks:**
- [X] Feature selection
  - [X] With variance threshold
  - [X] With K-best features
- [X] Data preprocessing
  - [X] Removing rows with missing values
  - [X] Removing duplicate rows
  - [X] Removing outliers (extreme values)
- [X] Naive bayes implementation 
- [ ] Extreme Gradient Boosting implementation

#### Data related:
https://papers.nips.cc/paper_files/paper/2004/file/5e751896e527c862bf67251a474b3819-Paper.pdf
https://papers.nips.cc/paper_files/paper/2004
https://competitions.codalab.org/competitions/6131
https://web.archive.org/web/20130503080434/http://www.nipsfsc.ecs.soton.ac.uk/evaluation/

Feature selection:
https://www.kaggle.com/code/ar2017/basics-of-feature-selection-with-python
https://www.kaggle.com/code/bbloggsbott/feature-selection-correlation-and-p-value
https://www.simonwenkel.com/2019/03/09/revisiting-ML-datasets-arcene.html

#### Extreme gradient boosting:
Understanding the theory:
https://www.nvidia.com/en-us/glossary/data-science/xgboost/
https://xgboost.readthedocs.io/en/stable/tutorials/model.html
https://www.xlstat.com/en/solutions/features/extreme-gradient-boosting-xgboost#:~:text=XGBOOST%2C%20which%20stands%20for%20%22Extreme,predict%20a%20target%2Fresponse%20variable
https://medium.com/analytics-vidhya/what-makes-xgboost-so-extreme-e1544a4433bb
https://arxiv.org/pdf/1603.02754.pdf

Implementation with libraries:
https://machinelearningmastery.com/extreme-gradient-boosting-ensemble-in-python/
https://gist.github.com/pb111/cc341409081dffa5e9eaf60d79562a03

From scratch implementation:
https://randomrealizations.com/posts/xgboost-from-scratch/
https://github.com/Ekeany/XGBoost-From-Scratch/blob/master/XGBoost.py
https://towardsdatascience.com/implementing-xgboost-from-scratch-6b7f2eb593c

#### Naive bayes:
The profs implementation can be used with some tweaks:
[Naive Bayes (Iris).pdf](Naive%20Bayes%20%28Iris%29.pdf)
